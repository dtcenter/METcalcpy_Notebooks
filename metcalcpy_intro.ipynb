{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b58a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "590ab4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metcalcpy==1.1.0\n",
      "  Using cached metcalcpy-1.1.0b5-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: metcalcpy\n",
      "  Attempting uninstall: metcalcpy\n",
      "    Found existing installation: metcalcpy 1.1.0b6.dev0\n",
      "    Uninstalling metcalcpy-1.1.0b6.dev0:\n",
      "      Successfully uninstalled metcalcpy-1.1.0b6.dev0\n",
      "Successfully installed metcalcpy-1.1.0b5\n"
     ]
    }
   ],
   "source": [
    "!pip install metcalcpy==1.1.0-b5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f69bd",
   "metadata": {},
   "source": [
    "# Import the METcalcpy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b0e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metcalcpy as cp\n",
    "from metcalcpy.util import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5830afc",
   "metadata": {},
   "source": [
    "# Explore what is available in the METcalcpy package's util module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac1c73bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module metcalcpy.util.utils in metcalcpy.util:\n",
      "\n",
      "NAME\n",
      "    metcalcpy.util.utils - Program Name: utils.py\n",
      "\n",
      "FUNCTIONS\n",
      "    aggregate_field_values(series_var_val, input_data_frame, line_type)\n",
      "        Finds and aggregates statistics for fields with values containing ';'.\n",
      "        Aggregation  happens by valid and lead times\n",
      "          These fields are coming from the scorecard and looks like this: vx_mask : ['EAST;NMT'].\n",
      "          This method finds these values and calculate aggregated stats for them\n",
      "        \n",
      "              Args:\n",
      "                  series_var_val: dictionary describing the series\n",
      "                  input_data_frame: Pandas DataFrame\n",
      "                  line_type: the line type\n",
      "        \n",
      "              Returns:\n",
      "                  Pandas DataFrame with aggregates statistics\n",
      "    \n",
      "    calc_derived_curve_value(val1, val2, operation)\n",
      "        Performs the operation with two numpy arrays.\n",
      "        Operations can be\n",
      "            'DIFF' - difference between elements of array 1 and 2\n",
      "            'RATIO' - ratio between elements of array 1 and 2\n",
      "            'SS' - skill score between elements of array 1 and 2\n",
      "            'SINGLE' - unchanged elements of array 1\n",
      "            'ETB' - Equivalence Testing Bounds of array 1 and 2\n",
      "        \n",
      "        Args:\n",
      "            val1:  array of floats\n",
      "            val2:  array of floats\n",
      "            operation: operation to perform\n",
      "        \n",
      "        Returns:\n",
      "             array\n",
      "            or None if one of arrays is None or one of the elements\n",
      "            is None or arrays have different size\n",
      "    \n",
      "    calc_series_sums(input_df, line_type)\n",
      "        Aggregates column values of the input data frame. Aggregation depends on the line type.\n",
      "        Following line types are currently supported : ctc, sl1l2, sal1l2, vl1l2,\n",
      "        val1l2, grad, nbrcnt, ecnt, rps\n",
      "        \n",
      "           Args:\n",
      "               input_df: input data as Pandas DataFrame\n",
      "               line_type: one of the supported line types\n",
      "           Returns:\n",
      "               Pandas DataFrame with aggregated values\n",
      "    \n",
      "    calculate_mtd_revision_stats(series_data: pandas.core.frame.DataFrame, lag_max: Union[int, NoneType] = None) -> dict\n",
      "        Calculates Mode-TD revision stats\n",
      "        :param series_data - DataFrame with columns 'stat_value' and 'revision_id'\n",
      "        :param  lag_max - maximum lag at which to calculate the acf.\n",
      "                    Can be an integer or None (the default).\n",
      "                    Default is 10*log10(N/m) where N is the number of\n",
      "                    observations and m the number of series.\n",
      "                    Will be automatically limited to one less than the number of\n",
      "                    observations in the series.\n",
      "        :return: a dictionary containing this statistics:\n",
      "                ww_run -  p-value of the Wald-Wolfowitz runs test\n",
      "                auto_cor_p - p-value of autocorrelation\n",
      "                auto_cor_r - estimated  autocorrelation for lag_max\n",
      "    \n",
      "    column_data_by_name(input_data, columns, column_name, rm_none=False) -> Union[list, NoneType]\n",
      "        Returns all values in the specified column. Removes None if requested\n",
      "        \n",
      "        Args:\n",
      "            input_data: 2-dimensional numpy array with data for the calculation\n",
      "                1st dimension - the row of data frame\n",
      "                2nd dimension - the column of data frame\n",
      "            columns: names of the columns for the 2nd dimension as Numpy array\n",
      "            column_name: the name of the column for SUM\n",
      "            rm_none: Should missing values (including non) be removed? Default - False\n",
      "        \n",
      "        Returns:\n",
      "            values of requested column or None\n",
      "    \n",
      "    column_data_by_name_value(input_data, columns, filters)\n",
      "        Filters  the input array by the criteria from the filters array\n",
      "        \n",
      "        Args:\n",
      "            input_data: 2-dimensional numpy array with data for the calculation\n",
      "                1st dimension - the row of data frame\n",
      "                2nd dimension - the column of data frame\n",
      "            columns: names of the columns for the 2nd dimension as Numpy array\n",
      "            filters: a dictionary of filters in 'column': 'value' format\n",
      "        \n",
      "        Returns:\n",
      "            filtered 2-dimensional numpy array\n",
      "    \n",
      "    compute_std_err_from_mean(data)\n",
      "        Function to compute the Standard Error of an uncorrelated time series using mean\n",
      "        Arg:\n",
      "            data: array of values presorted by date/time\n",
      "        \n",
      "        Returns: Standard Error, variance inflation factor flag, AR1 coefficient, the length of data\n",
      "    \n",
      "    compute_std_err_from_median_no_variance_inflation_factor(data)\n",
      "        Function to compute the Standard Error of an uncorrelated time series.\n",
      "        Remove the correlated portion of a time series, using a first order auto-correlation coefficient\n",
      "        to help compute an inflated variance factor for the uncorrelated portion of the time series.\n",
      "        Originator Rscript:  Eric Gilleland and Andrew Loughe, 08 JUL 2008\n",
      "        Arg:\n",
      "            data: array of values presorted by date/time\n",
      "        \n",
      "        Returns: Standard Error, variance inflation factor flag, AR1 coefficient, the length of data\n",
      "    \n",
      "    compute_std_err_from_median_variance_inflation_factor(data)\n",
      "        Function to compute the Standard Error of an uncorrelated time series\n",
      "        from median variance inflation factor.\n",
      "        Arg:\n",
      "            data: array of values presorted by date/time\n",
      "        \n",
      "        Returns: Standard Error, variance inflation factor flag, AR1 coefficient, the length of data\n",
      "    \n",
      "    compute_std_err_from_sum(data)\n",
      "        Function to compute the Standard Error of an uncorrelated time series\n",
      "        from sum.\n",
      "        Arg:\n",
      "            data: array of values presorted by date/time\n",
      "        \n",
      "        Returns: Standard Error, variance inflation factor flag, AR1 coefficient, the length of data\n",
      "    \n",
      "    convert_lon_360_to_180(longitude)\n",
      "        Convert a list or numpy array of longitudes from 0,360 to -180 to 180 (West-East)\n",
      "        \n",
      "        Args:\n",
      "        @params\n",
      "        \n",
      "        longitude: a numpy array or python list containing integer or float values from 0 to 360\n",
      "                   to be converted to values from -180 to 180\n",
      "        \n",
      "        Returns:\n",
      "            a numpy array containing values that range from -180 to 180 (West to East lons)\n",
      "            Maintains the input type, ie if longitudes are int, then the numpy array returned will\n",
      "            consist of int64.  If longitudes are float, then the returned numpy array will consist of\n",
      "            float.\n",
      "    \n",
      "    convert_lons_indices(lons_in, minlon_in, range_in)\n",
      "        Input:\n",
      "        @param lons_in: A list of longitudes to convert\n",
      "        @param minlon_in: The minimum value/starting value of converted longitudes\n",
      "        @param range_in: The number of longitudes to convert\n",
      "        \n",
      "        Returns:\n",
      "          reordered_lons:  sorted array of longitudes\n",
      "          lonsortlocs:  sorted array indices\n",
      "    \n",
      "    create_permutations(input_dict)\n",
      "        Create all permutations (ie cartesian products) between the\n",
      "        elements in the lists of dictionaries under the input_dict\n",
      "        dictionary:\n",
      "        \n",
      "        for example:\n",
      "        \n",
      "        input_dict:\n",
      "           model:\n",
      "             - GFS_0p25_G193\n",
      "           vx_mask:\n",
      "             - NH_CMORPH_G193\n",
      "             - SH_CMORPH_G193\n",
      "             - TROP_CMORPH_G193\n",
      "        \n",
      "        \n",
      "         So for the above case, we have two lists in the input_dict dictionary,\n",
      "         one for model and another for vx_mask:\n",
      "         model_list = [\"GFS_0p25_G193\"]\n",
      "         vx_mask_list = [\"NH_CMORPH_G193\", \"SH_CMORPH_G193\", \"TROP_CMORPH_G193\"]\n",
      "        \n",
      "         and a cartesian product representing all permutations of the lists\n",
      "         above results in the following:\n",
      "        \n",
      "        (\"GFS_0p25_G193\", \"NH_CMORPH_G193\")\n",
      "        (\"GFS_0p25_G193\", \"SH_CMORPH_G193\")\n",
      "        (\"GFS_0p25_G193\", \"TROP_CMORPH_G193\")\n",
      "        \n",
      "        Args:\n",
      "             input_dict: an input dictionary containing lists of values to\n",
      "                         permute\n",
      "        Returns:\n",
      "            permutation: a list of tuples that represent the possible\n",
      "            permutations of values in all lists\n",
      "    \n",
      "    create_permutations_mv(fields_values: Union[dict, list], index: int) -> list\n",
      "        Creates a list of all permutations of the dictionary or list values using METviewer logic\n",
      "        Input:\n",
      "        :param fields_values: dictionary of field-values, where values are lists\n",
      "            or a list of lists\n",
      "        :param index: the regression index\n",
      "        :return: the list of permutations\n",
      "    \n",
      "    equalize_axis_data(fix_vals_keys, fix_vals_permuted, params, input_data, axis='1')\n",
      "        Performs event equalisation on the specified axis on input data.\n",
      "        Args:\n",
      "            fix_vals_permuted - fixed values\n",
      "            params:        parameters for the statistic calculations  and data description\n",
      "            input_data:    data as DataFrame\n",
      "        Returns:\n",
      "            DataFrame with equalised data for the specified axis\n",
      "    \n",
      "    get_derived_curve_name(list_of_names)\n",
      "        Creates the derived series name from the list of series name components\n",
      "        \n",
      "         Args:\n",
      "             list_of_names: list of series name components\n",
      "                1st element - name of 1st series\n",
      "                2st element - name of 2st series\n",
      "                3st element - operation. Can be 'DIFF','RATIO', 'SS', 'SINGLE', 'ETB'\n",
      "        \n",
      "        Returns:\n",
      "            derived series name\n",
      "    \n",
      "    get_total_values(input_data, columns_names, aggregation)\n",
      "        Returns the total value for the given numpy array\n",
      "        \n",
      "        Args:\n",
      "            input_data: 2-dimensional numpy array with data for the calculation\n",
      "                1st dimension - the row of data frame\n",
      "                2nd dimension - the column of data frame\n",
      "            columns_names: names of the columns for the 2nd dimension as Numpy array\n",
      "            aggregation: if the aggregation on fields was performed\n",
      "        \n",
      "        Returns:\n",
      "                1 - if the aggregation was not preformed on the array\n",
      "                sum of all values from 'total' columns\n",
      "                    - if the aggregation was preformed on the array\n",
      "    \n",
      "    intersection(l_1, l_2)\n",
      "        Finds intersection between two lists\n",
      "        Args:\n",
      "            l_1: 1st list\n",
      "            l_2: 2nd list\n",
      "        \n",
      "        Returns:\n",
      "            list of intersection\n",
      "    \n",
      "    is_derived_point(point)\n",
      "        Determines if this point is a derived point\n",
      "        Args:\n",
      "            point: a list or tuple with point component values\n",
      "        \n",
      "        Returns:\n",
      "            True - if this point is derived\n",
      "            False - if this point is not derived\n",
      "    \n",
      "    is_string_integer(str_int)\n",
      "        Checks if the input string is integer.\n",
      "        \n",
      "         Args:\n",
      "             str_int: string value to check\n",
      "        \n",
      "        Returns:\n",
      "            True - if the input value is an integer\n",
      "            False - if the input value is not an integer\n",
      "    \n",
      "    nrow_column_data_by_name_value(input_data, columns, filters)\n",
      "        Calculates  the number of rows  that satisfy the criteria from the filters array\n",
      "        \n",
      "        Args:\n",
      "            input_data: 2-dimensional numpy array with data for the calculation\n",
      "                1st dimension - the row of data frame\n",
      "                2nd dimension - the column of data frame\n",
      "            columns: names of the columns for the 2nd dimension as Numpy array\n",
      "            filters: a dictionary of filters in 'column': 'value' format\n",
      "        \n",
      "        Returns:\n",
      "            calculated number of rows\n",
      "    \n",
      "    parse_bool(in_str)\n",
      "        Converts string to a boolean\n",
      "        Args:\n",
      "            in_str: a string that represents a boolean\n",
      "        \n",
      "        Returns:\n",
      "            boolean representation of the input string\n",
      "            ot string itself\n",
      "    \n",
      "    perfect_score_adjustment(mean_stats_1, mean_stats_2, statistic, pval)\n",
      "        Adjusts the perfect score depending on the statistic\n",
      "        \n",
      "        Args:\n",
      "            mean_stats_1: statistic value for the 1st point\n",
      "            mean_stats_2: statistic value for the 2nd point\n",
      "            statistic: name of the statistic\n",
      "            pval: perfect score\n",
      "        \n",
      "        \n",
      "        Returns:\n",
      "            Adjusted perfect score or None if statistic is unknown\n",
      "    \n",
      "    perform_event_equalization(params, input_data)\n",
      "        Performs event equalisation on input data. If there ara 2 axis:\n",
      "        perform EE on each and then on both\n",
      "        Args:\n",
      "            params:        parameters for the statistic calculations  and data description\n",
      "            input_data:    data as DataFrame\n",
      "        Returns:\n",
      "            DataFrame with equalised data\n",
      "    \n",
      "    pt(q, df, ncp=0, lower_tail=True)\n",
      "        Calculates the cumulative of the t-distribution\n",
      "        \n",
      "        Args:\n",
      "            q - vector of quantiles\n",
      "            df - degrees of freedom (> 0)\n",
      "            ncp - array_like shape parameters\n",
      "            lower_tail - if True (default), probabilities are P[X ≤ x], otherwise, P[X > x].\n",
      "        \n",
      "        Returns:\n",
      "            the cumulative of the t-distribution\n",
      "    \n",
      "    qt(p, df, ncp=0)\n",
      "        Calculates the quantile function of the t-distribution\n",
      "        \n",
      "         Args:\n",
      "            p - array_like quantiles\n",
      "            df - array_like shape parameters\n",
      "            ncp - array_like shape parameters\n",
      "        \n",
      "        Returns:\n",
      "            tquantile function of the t-distribution\n",
      "    \n",
      "    represents_int(possible_int)\n",
      "        Checks if the value is integer.\n",
      "        \n",
      "        Args:\n",
      "            possible_int: value to check\n",
      "        \n",
      "        Returns:\n",
      "            True - if the input value is an integer\n",
      "            False - if the input value is not an integer\n",
      "    \n",
      "    round_half_up(num, decimals=0)\n",
      "        The “rounding half up” strategy rounds every number to the nearest number\n",
      "           with the specified precision,\n",
      "        and breaks ties by rounding up.\n",
      "           Args:\n",
      "               n: a number\n",
      "               decimals: decimal place\n",
      "           Returns:\n",
      "               rounded number\n",
      "    \n",
      "    sum_column_data_by_name(input_data, columns, column_name, rm_none=True)\n",
      "        Calculates  SUM of all values in the specified column\n",
      "        \n",
      "        Args:\n",
      "            input_data: 2-dimensional numpy array with data for the calculation\n",
      "                1st dimension - the row of data frame\n",
      "                2nd dimension - the column of data frame\n",
      "            columns: names of the columns for the 2nd dimension as Numpy array\n",
      "            column_name: the name of the column for SUM\n",
      "            rm_none: Should missing values (including non) be removed? Default - True\n",
      "        \n",
      "        Returns:\n",
      "            calculated SUM as float\n",
      "            or None if all of the data values are non\n",
      "    \n",
      "    tost_paired(n: int, m1: float, m2: float, sd1: float, sd2: float, r12: float, low_eqbound_dz: float, high_eqbound_dz: float, alpha: float = None) -> dict\n",
      "        TOST function for a dependent t-test (Cohen's dz). Based on Rscript function TOSTpaired\n",
      "        \n",
      "        Args:\n",
      "            n: sample size (pairs)\n",
      "            m1: mean of group 1\n",
      "            m2: mean of group 2\n",
      "            sd1: standard deviation of group 1\n",
      "            sd2: standard deviation of group 2\n",
      "            r12: correlation of dependent variable between group 1 and group 2\n",
      "            low_eqbound_dz: lower equivalence bounds (e.g., -0.5) expressed in standardized mean difference (Cohen's dz)\n",
      "            high_eqbound_dz: upper equivalence bounds (e.g., 0.5) expressed in standardized mean difference (Cohen's dz)\n",
      "            alpha: alpha level (default = 0.05)\n",
      "        \n",
      "        Returns:\n",
      "            Returns a dictionary with calculated TOST values\n",
      "                    dif - Mean Difference\n",
      "                    t - TOST t-values 1 and 2 as a tuple\n",
      "                    p - TOST p-values and 2 as a tuple\n",
      "                    degrees_of_freedom - degrees of freedom\n",
      "                    ci_tost - confidence interval TOST Lower and Upper limit as a tuple\n",
      "                    ci_ttest - confidence interval TTEST Lower and Upper limit as a tuple\n",
      "                    eqbound - equivalence bound low and high as a tuple\n",
      "                    xlim - limits for x-axis\n",
      "                    combined_outcome - outcome\n",
      "                    test_outcome - pt test outcome\n",
      "                    tist_outcome - TOST outcome\n",
      "    \n",
      "    unique(in_list)\n",
      "        Extracts unique values from the list.\n",
      "        Args:\n",
      "            in_list: list of values\n",
      "        \n",
      "        Returns:\n",
      "            list of unique elements from the input list\n",
      "\n",
      "DATA\n",
      "    CODE_TO_OUTCOME_TO_MESSAGE = {'diff_eqv': 'statistically different fro...\n",
      "    DATE_TIME_REGEX = r'\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}'\n",
      "    GROUP_SEPARATOR = ':'\n",
      "    OPERATION_TO_SIGN = {'DIFF': '-', 'DIFF_SIG': '-', 'ETB': ' and ', 'RA...\n",
      "    PRECISION = 7\n",
      "    STR_TO_BOOL = {'False': False, 'True': True}\n",
      "    THREE_D_DATA_FILTER = {'object_type': '3d'}\n",
      "    TWO_D_DATA_FILTER = {'object_type': '2d'}\n",
      "    Union = typing.Union\n",
      "    nct = <scipy.stats._continuous_distns.nct_gen object>\n",
      "    t = <scipy.stats._continuous_distns.t_gen object>\n",
      "\n",
      "AUTHOR\n",
      "    Tatiana Burek\n",
      "\n",
      "FILE\n",
      "    /opt/miniconda3/envs/jupyter/lib/python3.8/site-packages/metcalcpy/util/utils.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511586ca",
   "metadata": {},
   "source": [
    "# Let's experiment with the create_permutations function in the utils.py module to create permutations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b808decd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Model_A', 'ATL'),\n",
       " ('Model_A', 'NHemi'),\n",
       " ('Model_A', 'SHemi'),\n",
       " ('Model_A', 'Pacific'),\n",
       " ('Model_B', 'ATL'),\n",
       " ('Model_B', 'NHemi'),\n",
       " ('Model_B', 'SHemi'),\n",
       " ('Model_B', 'Pacific'),\n",
       " ('Model_C', 'ATL'),\n",
       " ('Model_C', 'NHemi'),\n",
       " ('Model_C', 'SHemi'),\n",
       " ('Model_C', 'Pacific')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['Model_A', 'Model_B', 'Model_C']\n",
    "vx_masking_regions =['ATL','NHemi', 'SHemi','Pacific']\n",
    "input_list = []\n",
    "input_list.append(models)\n",
    "input_list.append(vx_masking_regions)\n",
    "    \n",
    "permutations = utils.create_permutations(input_list)\n",
    "permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b555a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
